{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import expit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return expit(z)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "def predict(features, weights):\n",
    "    z = np.dot(features, weights)\n",
    "    return sigmoid(z)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "def update_weights(features, labels, weights, lr):\n",
    "    N = len(features)\n",
    "\n",
    "    predictions = predict(features, weights)\n",
    "\n",
    "    gradient = lr * (np.dot(features.T, predictions - labels) / N)\n",
    "\n",
    "    weights -= gradient\n",
    "\n",
    "    return weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "def decision_boundary(prob):\n",
    "    return 1 if prob >= 0.5 else 0\n",
    "\n",
    "def classify(predictions):\n",
    "    decision_boundarys = np.vectorize(decision_boundary)\n",
    "    return decision_boundarys(predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "def train(features, labels, weights, lr, iters):\n",
    "    for i in range(iters):\n",
    "        weights = update_weights(features, labels, weights, lr)\n",
    "\n",
    "    return weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [],
   "source": [
    "def accuracy(predicted_labels, actual_labels):\n",
    "    diff = predicted_labels - actual_labels\n",
    "    return 1.0 - (float(np.count_nonzero(diff)) / len(diff))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "def precision(predicted_labels, actual_labels):\n",
    "    connected_result = zip(predicted_labels, actual_labels.values)\n",
    "    both_ones = 0\n",
    "    first_one = 0\n",
    "    for prediction, actual in connected_result:\n",
    "        if (prediction[0] == 1) & (actual[0] == 1):\n",
    "            both_ones += 1\n",
    "        if (prediction[0] == 1) & (actual[0] != 1):\n",
    "            first_one += 1\n",
    "\n",
    "    TP = both_ones\n",
    "    FP = first_one\n",
    "\n",
    "    return (TP)/ (TP+FP)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "def recall(predicted_labels, actual_labels):\n",
    "    connected_result = zip(predicted_labels, actual_labels.values)\n",
    "    both_ones = 0\n",
    "    total_ones = 0\n",
    "    for prediction, actual in connected_result:\n",
    "        if (prediction[0] == 1) & (actual[0] == 1):\n",
    "            both_ones += 1\n",
    "        if (actual[0] == 1):\n",
    "            total_ones += 1\n",
    "\n",
    "    return both_ones / total_ones"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [],
   "source": [
    "def f1_score(predicted_labels, actual_labels):\n",
    "    precision_score = precision(predicted_labels, actual_labels)\n",
    "    recall_score = recall(predicted_labels, actual_labels)\n",
    "\n",
    "    return (2 * precision_score * recall_score) / (precision_score + recall_score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [],
   "source": [
    "def specificity(predicted_labels, actual_labels):\n",
    "    connected_result = zip(predicted_labels, actual_labels.values)\n",
    "    both_zeroes = 0\n",
    "    ones_zeroes = 0\n",
    "    for prediction, actual in connected_result:\n",
    "        if (prediction[0] == 0) & (actual[0] == 0):\n",
    "            both_zeroes += 1\n",
    "        if (prediction[0] == 1 & actual[0] == 0):\n",
    "            ones_zeroes += 1\n",
    "\n",
    "    return both_zeroes / (both_zeroes + ones_zeroes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/ragemachine/DataspellProjects/ml-assignments/assignment-2/Diabetes.csv\")\n",
    "features = ['Pregnency', 'Glucose', 'Blood Pressure', 'Skin Thickness', 'Insulin', 'BMI', 'DFP', 'Age']\n",
    "x = df.loc[:, features]\n",
    "y = df.loc[:, ['Diabetes']]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x,y, random_state=0, train_size=0.7)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7056277056277056\n",
      "Precision = 0.5238095238095238\n",
      "Recall = 0.6111111111111112\n",
      "F1 Score = 0.5641025641025642\n",
      "Specificity Score = 0.5\n"
     ]
    }
   ],
   "source": [
    "weights = np.zeros([8, 1])\n",
    "weights = train(X_train, Y_train, weights, 0.1, 1000)\n",
    "predictions = predict(X_test, weights)\n",
    "classifications = classify(predictions)\n",
    "\n",
    "accuracy_result = accuracy(classifications, Y_test)\n",
    "precision_result = precision(classifications, Y_test)\n",
    "recall_result = recall(classifications, Y_test)\n",
    "f1_result = f1_score(classifications, Y_test)\n",
    "specificity_result = specificity(classifications, Y_test)\n",
    "\n",
    "print(\"Accuracy = {0}\".format(str(accuracy_result)))\n",
    "print(\"Precision = {0}\".format(str(precision_result)))\n",
    "print(\"Recall = {0}\".format(str(recall_result)))\n",
    "print(\"F1 Score = {0}\".format(str(f1_result)))\n",
    "print(\"Specificity Score = {0}\".format(str(specificity_result)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}